# PyTriton

[PyTriton](https://github.com/triton-inference-server/pytriton) enables fast deployment of AI/ML models and python tools in a [Flask](https://flask.palletsprojects.com/) API like manner. PyTriton can deploy model inference calls as commonly found in many github repos. [Triton](https://github.com/triton-inference-server/server) is already built-in the pytriton inference server so models in torch, onnx, tf2 and tensorrt are also supported. 

PyTriton is like HuggingFace model hub except that we can deploy models on our local machines therefore enabling faster inference time and higher flexibility in terms of configuration.

## Table of Contents

1. [Segment Anything Model (SAM)](https://github.com/roatienza/mlops/tree/main/triton/sam)
2. [Open CLIP and CoCa](https://github.com/roatienza/mlops/blob/main/triton/openclip)
