{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset and Dataloader Demo\n",
    "\n",
    "We illustrate how to build a custom dataset and dataloader. We will use our collected and labeled images for object detection. The `640x480` RGB images were collected using an off-the-shelf USB camera (A4TECH PK-635G).\n",
    "The images were labeled using [VIA](https://www.robots.ox.ac.uk/~vgg/software/via/) and the labels are stored in a CSV file. \n",
    "\n",
    "Please download the dataset from [here](https://bit.ly/adl2-ssd). Extract the dataset on the same directory as this file:\n",
    "\n",
    "```\n",
    "--> datasets --> python --> config.py\n",
    "                        --> dataloader_demo.ipynb\n",
    "                        --> drinks/\n",
    "                        --> label_utils.py\n",
    "                        --> sample_labels.png\n",
    "                        ...\n",
    "```\n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` account. See our discussion on [`wandb.ai`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image annotation\n",
    "\n",
    "A sample image annotation is shown below. There are only 3 categories: `Water`, `Soda`, and `Juice`. By default the backgroud is the first category.\n",
    "\n",
    "<img src=\"sample_labels.png\" width=\"640\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules.\n",
    "\n",
    "[`label_utils`](label_utils.py) is a helper module for loading the CSV file and converting the labels to string. It also contains helper functions to build the label dictionary from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import label_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Login to and initialize** `wandb`. You will need to use your `wandb` API key to run this demo.\n",
    "\n",
    "We will use the following dataset and dataloader configuration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 11:04:47.264144: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-18 11:04:47.264187: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/datasets/python/wandb/run-20220318_110444-vjogcd0e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/dataloader-project/runs/vjogcd0e\" target=\"_blank\">dutiful-frog-4</a></strong> to <a href=\"https://wandb.ai/upeee/dataloader-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": 8,\n",
    "    \"dataset\": \"drinks\",\n",
    "    \"train_split\": \"drinks/labels_train.csv\",\n",
    "    \"test_split\": \"drinks/labels_test.csv\",}\n",
    "run = wandb.init(project=\"dataloader-project\", entity=\"upeee\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader for Custom Object Detection\n",
    "\n",
    "The dataset is a list of images and their labels. The image filenames and their labels are stored in a CSV file using the following format.\n",
    "\n",
    "```\n",
    "frame,xmin,xmax,ymin,ymax,class_id\n",
    "0001000.jpg,310,445,104,443,1\n",
    "0000999.jpg,194,354,96,478,1\n",
    "0000998.jpg,105,383,134,244,1\n",
    "0000997.jpg,157,493,89,194,1\n",
    "0000996.jpg,51,435,207,347,1\n",
    "...\n",
    "```\n",
    "\n",
    "We will build a dictionary of `path_to_image` to `label` mapping. The `label` is a tensor of the form `xmin,xmax,ymin,ymax,class_id`. There can be multiple labels for an image since there can be multiple objects in an image.\n",
    "\n",
    "The `ImageDataset` class is a custom dataset class that loads the images and labels using the dictionary. The `ImageDataset` class is a subclass of `torch.utils.data.Dataset`.\n",
    "\n",
    "Our train and test dataloaders use the `wandb` configuration. \n",
    "\n",
    "We also create a custom `collate_fn` function to handle the labels per image. `collate_fn` pads all labels in a mini-batch to the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict, test_classes = label_utils.build_label_dictionary(config['test_split'])\n",
    "train_dict, train_classes  = label_utils.build_label_dictionary(config['train_split'])\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dictionary, transform=None):\n",
    "        self.dictionary = dictionary\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = list(self.dictionary.keys())[idx]\n",
    "        boxes = self.dictionary[key]\n",
    "        img = Image.open(key)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, boxes\n",
    "\n",
    "train_split = ImageDataset(train_dict, transforms.ToTensor())\n",
    "test_split = ImageDataset(test_dict, transforms.ToTensor())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    maxlen = max([len(x[1]) for x in batch])    \n",
    "    images = []\n",
    "    boxes = []\n",
    "    for i in range(len(batch)):\n",
    "        img, box = batch[i]\n",
    "        images.append(img)\n",
    "        # pad with zeros if less than maxlen\n",
    "        if len(box) < maxlen:\n",
    "            box = np.concatenate((box, np.zeros((maxlen-len(box), box.shape[-1]))), axis=0)\n",
    "    \n",
    "        box = torch.from_numpy(box)\n",
    "        boxes.append(box)\n",
    "\n",
    "    return torch.stack(images, 0), torch.stack(boxes, 0)\n",
    "\n",
    "train_loader = DataLoader(train_split, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          num_workers=config['num_workers'], \n",
    "                          pin_memory=config['pin_memory'],\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_split,\n",
    "                         batch_size=config['batch_size'],\n",
    "                         shuffle=False,\n",
    "                         num_workers=config['num_workers'],\n",
    "                         pin_memory=config['pin_memory'],\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sample data from train split\n",
    "\n",
    "We visualize sample images from the train split by creating a table with one column to visualize an image and the objects using bounding boxes. The annotation is stored in a list of dictionaries. One dictionary per image using `position`, `class_id`, `domain` and `box_caption` as keys. Please check the [`wandb` media documentation](https://docs.wandb.ai/guides/track/log/media) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/upeee/dataloader-project/runs/vjogcd0e?jupyter=true\" style=\"border:none;width:100%;height:1000px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d77bcb9d02e417886a02909ea3f4bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.730 MB of 3.730 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.999969â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dutiful-frog-4</strong>: <a href=\"https://wandb.ai/upeee/dataloader-project/runs/vjogcd0e\" target=\"_blank\">https://wandb.ai/upeee/dataloader-project/runs/vjogcd0e</a><br/>Synced 7 W&B file(s), 1 media file(s), 10 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220318_110444-vjogcd0e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, boxes = next(iter(train_loader))\n",
    "class_labels = {i : label_utils.index2class(i) for i in train_classes}\n",
    "\n",
    "run.display(height=1000)\n",
    "table = wandb.Table(columns=['Image'])\n",
    "\n",
    "for image, box in zip(images, boxes):\n",
    "    dict = []\n",
    "    for i in range(box.shape[0]):\n",
    "        if box[i, -1] == 0:\n",
    "            continue\n",
    "        dict_item = {}\n",
    "        dict_item[\"position\"] = {\n",
    "            \"minX\": box[i, 0].item(),\n",
    "            \"maxX\": box[i, 1].item(),\n",
    "            \"minY\": box[i, 2].item(),\n",
    "            \"maxY\": box[i, 3].item(),\n",
    "            }   \n",
    "        dict_item[\"domain\"] =  \"pixel\"\n",
    "        dict_item[\"class_id\"] = (int)(box[i, 4].item())\n",
    "        dict_item[\"box_caption\"] = label_utils.index2class(dict_item[\"class_id\"])\n",
    "        dict.append(dict_item)\n",
    "    \n",
    "    img = wandb.Image(image, boxes={\n",
    "        \"ground_truth\": {\n",
    "            \"box_data\": dict,\n",
    "            \"class_labels\": class_labels\n",
    "            }\n",
    "        })\n",
    "    table.add_data(img)\n",
    "\n",
    "wandb.log({\"train_loader\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
