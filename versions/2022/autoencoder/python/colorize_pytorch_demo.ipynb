{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colorization AutoEncoder PyTorch Demo using CIFAR10\n",
    "\n",
    "In this demo, we build a simple colorization autoencoder using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 20:41:40.216074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from einops import rearrange, reduce\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Encoder using PyTorch\n",
    "\n",
    "We use 3 CNN layers to encode the grayscale input image. We use stride of 2 to reduce the feature map size. The last MLP layer resizes the flattened feature map to the target latent vector size. We use more filters and a much bigger latent vector size of 256 to encode more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features=1, kernel_size=3, n_filters=64, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_features, n_filters, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters*2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_filters*2, n_filters*4, kernel_size=kernel_size, stride=2)\n",
    "        self.fc1 = nn.Linear(2304, feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = nn.ReLU()(self.conv1(x))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.ReLU()(self.conv3(y))\n",
    "        y = rearrange(y, 'b c h w -> b (c h w)')\n",
    "\n",
    "        y = self.fc1(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# use this to get the correct input shape for  fc1. \n",
    "encoder = Encoder(n_features=1)\n",
    "x = torch.Tensor(1, 1, 32, 32)\n",
    "h = encoder(x)\n",
    "print(\"h.shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Decoder using PyTorch\n",
    "\n",
    "A decoder is used to reconstruct the input image. The decoder is trained to reconstruct the input data from the latent space. The architecture is similar to the encoder but inverted. A latent vector is resized using an MLP layer so that it is suitable for a convolutional layer. We use strided tranposed convolutional layers to upsample the feature map until the desired image size is reached. The target image is the colorized version of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tilde.shape: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, n_filters=256, feature_dim=256, output_size=32, output_channels=3):\n",
    "        super().__init__()\n",
    "        self.init_size = output_size // 2**2 \n",
    "        self.fc1 = nn.Linear(feature_dim, self.init_size**2 * n_filters)\n",
    "        # output size of conv2dtranspose is (h-1)*2 + 1 + (kernel_size - 1)\n",
    "        self.conv1 = nn.ConvTranspose2d(n_filters, n_filters//2, kernel_size=kernel_size, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(n_filters//2, n_filters//4, kernel_size=kernel_size, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(n_filters//4, n_filters//4, kernel_size=kernel_size, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose2d(n_filters//4, output_channels, kernel_size=kernel_size+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, _ = x.shape\n",
    "        y = self.fc1(x)\n",
    "        y = rearrange(y, 'b (c h w) -> b c h w', b=B, h=self.init_size, w=self.init_size)\n",
    "        y = nn.ReLU()(self.conv1(y))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.ReLU()(self.conv3(y))\n",
    "        y = nn.Sigmoid()(self.conv4(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "decoder = Decoder()\n",
    "x_tilde = decoder(h)\n",
    "print(\"x_tilde.shape:\", x_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning Colorization AutoEncoder\n",
    "\n",
    "In the colorization autoencoder, the encoder extracts features from the input image and the decoder reconstructs the input image from the latent space. The decoder adds color. The decoder's last layer has 3 output channels corresponding to RGB.\n",
    "\n",
    "We `gray_collate_fn` to generate gray images from RGB images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        xn = reduce(x,\"b c h w -> b 1 h w\", 'mean')\n",
    "        return xn, x\n",
    "\n",
    "class LitColorizeCIFAR10Model(LightningModule):\n",
    "    def __init__(self, feature_dim=256, lr=0.001, batch_size=64,\n",
    "                 num_workers=4, max_epochs=30, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(feature_dim=feature_dim)\n",
    "        self.decoder = Decoder(feature_dim=feature_dim)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x_tilde = self.decoder(h)\n",
    "        return x_tilde\n",
    "\n",
    "    # this is called during fit()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    # calls to self.log() are recorded in wandb\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "\n",
    "    # this is called at the end of an epoch\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"x_in\" : x_in, \"x\": x, \"x_tilde\" : x_tilde, \"test_loss\" : loss,}\n",
    "\n",
    "    # this is called at the end of all epochs\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # validation is the same as test\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "    # we use Adam optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler], \n",
    "    \n",
    "    # this is called after model instatiation to initiliaze the datasets and dataloaders\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataloader()\n",
    "        self.test_dataloader()\n",
    "\n",
    "    # build train and test dataloaders using MNIST dataset\n",
    "    # we use simple ToTensor transform\n",
    "    def train_dataloader(self):        \n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.CIFAR10(\n",
    "                \"./data\", train=True, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=gray_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.CIFAR10(\n",
    "                \"./data\", train=False, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=gray_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments\n",
    "\n",
    "Similar to the MNIST AE but we use a bigger latent vector size of 256 given that the colorization task needs more feature information from the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"PyTorch Lightning Colorization AE CIFAR10 Example\")\n",
    "    parser.add_argument(\"--max-epochs\", type=int, default=30, help=\"num epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--feature-dim\", type=int, default=256, help=\"ae feature dimension\")\n",
    "\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"num workers\")\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases Callback\n",
    "\n",
    "The callback logs train and validation metrics to `wandb`. It also logs sample predictions. This is similar to our `WandbCallback` example for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # process first 10 images of the first batch\n",
    "        if batch_idx == 0:\n",
    "            x, c = batch\n",
    "            n = pl_module.hparams.batch_size // 4\n",
    "            outputs = outputs[\"x_tilde\"]\n",
    "            columns = ['gt color', 'gray', 'colorized']\n",
    "            data = [[wandb.Image(c_i), wandb.Image(x_i), wandb.Image(x_tilde_i)] for c_i, x_i, x_tilde_i in list(zip(c[:n], x[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key=\"cifar10-colorize-ae\", columns=columns, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an AE\n",
    "\n",
    "We train the autoencoder on the CIFAR10 dataset. \n",
    "The results can be viewed on [wandb](https://app.wandb.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    ae = LitColorizeCIFAR10Model(feature_dim=args.feature_dim, lr=args.lr, \n",
    "                                 batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                                 max_epochs=args.max_epochs)\n",
    "    #ae.setup()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-05-25 20:41:45.914314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/autoencoder/python/wandb/run-20220525_204144-3me3rcz0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rowel/colorize-cifar10/runs/3me3rcz0\" target=\"_blank\">copper-night-1</a></strong> to <a href=\"https://wandb.ai/rowel/colorize-cifar10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 959 K \n",
      "1 | decoder | Decoder | 4.6 M \n",
      "2 | loss    | MSELoss | 0     \n",
      "------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.317    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb69619143dd43de82492611e8941aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99f4e8fa8054108a9d544eda73ffcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ecf0a330934db3a4bb91bcedd87dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64990f60e5246cf95d25a964338bc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45b980f98db44dab4386a6db0a9125c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c025845b33241e4a2e3560813f22cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66e0099c76547278db1fb25272dd94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7002c8b9ee7b43c4bda2899f4044b2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35fe07e9a1941cbb64fc95ba32455c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8736022b05f41eb9e2d4147359dcb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eebbd52c94647ceba60d59192ce5557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e80ee515e6f45ad80972bf4314900da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf766db5073462aba2b3dc16ec70b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b146588a0ca5489e806f3cd139a8fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121e267803a74ee79d649903f1ee8b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c5d64b84a400081237f710439281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e0ef74acd34c94a16a4c8f7a84e38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955480f2c594404da65b37d33991178a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcb0d85f7cf4c45a9f83ad230a9eb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85354c020b3e4de3aaf5d5b8b18042ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ce04b22e8b4d6ca5f341b6d2e8ff78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc471ebd71c8466f833788b740e37196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6258d883b2684adbb59f5831ab5125d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb4bab522ce45dcb1d455de324ab87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb7f5bed134e7693515f2ee2200cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975701b7899f4c428b28efba0ecae226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe49f70d3ff048ee9b8c66d77494e735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7ddb73eb4542dbbb988449fb97fdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bca003652684589919dc7ff9f28df4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3ebbfce3bc469f99cf9413d73c1ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f32a9f761d14b8dadad94cc16bf06bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc56c9d4d3f64b4b9a617cf67f79fae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 361.66529393196106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2a357a9f81418b982d44bf0ddbc7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.470 MB of 1.470 MB uploaded (0.069 MB deduped)\\r'), FloatProgress(value=0.999662…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 4.6%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▅▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>test_loss</td><td>0.00777</td></tr><tr><td>train_loss</td><td>0.00365</td></tr><tr><td>trainer/global_step</td><td>23459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">copper-night-1</strong>: <a href=\"https://wandb.ai/rowel/colorize-cifar10/runs/3me3rcz0\" target=\"_blank\">https://wandb.ai/rowel/colorize-cifar10/runs/3me3rcz0</a><br/>Synced 7 W&B file(s), 31 media file(s), 559 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_204144-3me3rcz0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    wandb_logger = WandbLogger(project=\"colorize-cifar10\")\n",
    "    start_time = time.time()\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger,\n",
    "                      callbacks=[WandbCallback()])\n",
    "    trainer.fit(ae)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: {}\".format(elapsed_time))\n",
    "\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
