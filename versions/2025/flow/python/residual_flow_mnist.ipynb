{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Flow Example\n",
    "\n",
    "Simple Normalizing Flow for MNIST using Residual Blocks in PyTorch.\n",
    "\n",
    "What it does:\n",
    "- Implements a residual block\n",
    "- Implements a normalizing flow using these blocks\n",
    "- Trains on MNIST by maximizing log-likelihood\n",
    "- Generates and saves sample images during training\n",
    "\n",
    "Dependencies:\n",
    "- torch, torchvision, matplotlib\n",
    "\n",
    "\n",
    "This implementation is intentionally small and educational rather than highly-optimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from typing import List\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h: int, w: int, invert: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Create a binary checkerboard mask of shape (1, h, w).\"\"\"\n",
    "    mask = [[((i + j) % 2) for j in range(w)] for i in range(h)]\n",
    "    # to numpy\n",
    "    mask = torch.from_numpy(np.array(mask)).float()\n",
    "    if invert:\n",
    "        mask = 1.0 - mask\n",
    "    return mask.unsqueeze(0)  # (1, h, w)\n",
    "\n",
    "# For a flattened image we'll produce a mask vector of length 784\n",
    "def mask_flat_from_checkerboard(h=28, w=28, invert=False) -> torch.Tensor:\n",
    "    return create_checkerboard_mask(h, w, invert).view(-1)  # (784,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block\n",
    "\n",
    "Residual block with skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        # Use smaller initialization for better Lipschitz constraint\n",
    "        for m in self.g.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "        self.scale = nn.Parameter(torch.tensor(-2.0))  # Start with small scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply scaled residual function with bounded scaling\n",
    "        scale = torch.sigmoid(self.scale) * 0.5  # Keep scale in [0, 0.5]\n",
    "        g_x = self.g(x) * scale\n",
    "        y = x + g_x\n",
    "        \n",
    "        # Hutchinson's trace estimator with multiple samples for stability\n",
    "        num_samples = 1\n",
    "        trace_est = 0\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            epsilon = torch.randn_like(x)\n",
    "            \n",
    "            jvp = torch.autograd.grad(\n",
    "                outputs=g_x, \n",
    "                inputs=x, \n",
    "                grad_outputs=epsilon, \n",
    "                create_graph=self.training,\n",
    "                retain_graph=True\n",
    "            )[0]\n",
    "            \n",
    "            # tr(J_g) ≈ ε^T * J_g * ε\n",
    "            trace_est += torch.sum(epsilon * jvp, dim=1)\n",
    "        \n",
    "        trace_est = trace_est / num_samples\n",
    "        \n",
    "        # For small Lipschitz: log|det(I + J_g)| ≈ tr(J_g) - 0.5 * tr(J_g)^2 + ...\n",
    "        # Using first-order approximation for stability\n",
    "        logdet = trace_est\n",
    "        \n",
    "        return y, logdet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The Normalizing Flow model consists of multiple residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFlow(nn.Module):\n",
    "    def __init__(self, dim=784, hidden_dim=512, n_blocks=4):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(dim, hidden_dim) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        logdet_sum = torch.zeros(x.size(0), device=x.device)\n",
    "        z = x  # Don't recreate tensor - use input directly\n",
    "        for block in self.blocks:\n",
    "            z, logdet = block(z)\n",
    "            logdet_sum += logdet\n",
    "        return z, logdet_sum\n",
    "\n",
    "    def inverse(self, z, n_iter=100, tol=1e-6):\n",
    "        \"\"\"Fixed-point iteration with optional convergence check.\"\"\"\n",
    "        x = z.clone()\n",
    "        for i in range(n_iter):\n",
    "            x_prev = x\n",
    "            for block in reversed(self.blocks):\n",
    "                x = x - block.g(x) * torch.tanh(block.scale)\n",
    "            \n",
    "            # Optional: check convergence\n",
    "            if i > 10 and torch.norm(x - x_prev) < tol:\n",
    "                break\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The Normalizing Flow model is trained by maximizing the log-likelihood of the training data. The training loop iterates over the dataset, computes the negative log-likelihood loss, and updates the model parameters using the Adam optimizer. The model's performance is monitored by generating and saving sample images at regular intervals during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualFlow(\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x ResidualBlock(\n",
      "      (g): Sequential(\n",
      "        (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=784, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of model parameters: 0.87 million\n",
      "Running on: cuda\n",
      "Epoch 0, avg loss: nan, lr: 0.000100\n",
      "Epoch 1, avg loss: nan, lr: 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m flow\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     36\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/datasets/mnist.py:143\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m img \u001b[38;5;241m=\u001b[39m _Image_fromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "dim = 28 * 28\n",
    "n_blocks = 4\n",
    "hidden_dim = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: t.view(-1)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='~/data', \n",
    "                                           train=True, \n",
    "                                           download=True, \n",
    "                                           transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "flow = ResidualFlow(dim=dim, hidden_dim=hidden_dim, n_blocks=n_blocks).to(device)\n",
    "\n",
    "print(flow)\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in flow.parameters())/1e6:.2f} million\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(flow.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "def gaussian_log_prob(z):\n",
    "    return -0.5 * (z.pow(2).sum(dim=1) + z.size(1) * math.log(2 * math.pi))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    flow.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z, logdet = flow(x)\n",
    "        \n",
    "        log_pz = gaussian_log_prob(z)\n",
    "        loss = -(log_pz + logdet).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch}, avg loss: {total_loss / (batch_idx + 1):.4f}, lr: {lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "The trained Normalizing Flow model can be evaluated by generating new samples from the learned distribution. This is done by sampling from a standard normal distribution and passing the samples through the inverse of the flow to obtain images in the original data space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(16, 784).to(device)\n",
    "    # Use more iterations for better inverse approximation\n",
    "    x_gen = flow.inverse(z, n_iter=200)\n",
    "    \n",
    "    # Clamp values to valid range [0, 1]\n",
    "    x_gen = torch.clamp(x_gen, 0, 1)\n",
    "    \n",
    "    imgs = x_gen.view(-1, 1, 28, 28).cpu()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(imgs[i,0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Residual Flow Generated MNIST Samples\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/residual_flow_samples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "dim = 28 * 28\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # maps to [0,1]\n",
    "    # transforms.Lambda(lambda t: (t - 0.5) * 2),  # scale to [-1,1]\n",
    "    transforms.Lambda(lambda t: t.view(-1)),  # flatten to 784\n",
    "])\n",
    "# Load the MNIST train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='~/data', \n",
    "                                           train=True, \n",
    "                                           download=True, \n",
    "                                           transform=transform)\n",
    "\n",
    "# Create the train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='~/data', \n",
    "                                          train=False, \n",
    "                                          download=True, \n",
    "                                          transform=transform)\n",
    "\n",
    "# Create the test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# -----------------------\n",
    "# Residual Block\n",
    "# -----------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        self.scale = nn.Parameter(torch.zeros(1))  # stability scaling\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the residual function with scaling\n",
    "        g_x = self.g(x) * torch.tanh(self.scale)\n",
    "        y = x + g_x\n",
    "        \n",
    "        # Compute log determinant using Hutchinson's trace estimator\n",
    "        # log|det(I + J_g)| ≈ tr(J_g) for small Lipschitz constant\n",
    "        epsilon = torch.randn_like(x)\n",
    "        \n",
    "        # Compute J_g * epsilon using vector-Jacobian product\n",
    "        g_x_eps = torch.autograd.grad(\n",
    "            outputs=g_x, \n",
    "            inputs=x, \n",
    "            grad_outputs=epsilon, \n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        # Hutchinson's estimator: tr(J_g) ≈ ε^T J_g ε\n",
    "        trace_est = torch.sum(epsilon * g_x_eps, dim=1)\n",
    "        \n",
    "        # log|det(I + J_g)| ≈ tr(J_g) for contractive g\n",
    "        logdet = trace_est\n",
    "        \n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Residual Flow Model\n",
    "# -----------------------\n",
    "class ResidualFlow(nn.Module):\n",
    "    def __init__(self, dim=784, hidden_dim=512, n_blocks=4):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(dim, hidden_dim) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        logdet_sum = torch.zeros(x.size(0), device=x.device)\n",
    "        z = x.requires_grad_(True)  # Enable gradient tracking for Hutchinson estimator\n",
    "        for block in self.blocks:\n",
    "            z, logdet = block(z)\n",
    "            logdet_sum += logdet\n",
    "        return z, logdet_sum\n",
    "\n",
    "    def inverse(self, z, n_iter=100):\n",
    "        # Fixed-point iteration for inverse - needs more iterations for convergence\n",
    "        x = z\n",
    "        for _ in range(n_iter):\n",
    "            for block in reversed(self.blocks):\n",
    "                # x_{t-1} = x_t - g(x_t) * scale\n",
    "                x = x - block.g(x) * torch.tanh(block.scale)\n",
    "        return x\n",
    "\n",
    "\n",
    "flow = ResidualFlow(dim=784, hidden_dim=128, n_blocks=4).to(device)\n",
    "\n",
    "print(flow)\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in flow.parameters())/1e6:.2f} million\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Define the optimizer and the scheduler\n",
    "optimizer = torch.optim.Adam(flow.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "def gaussian_log_prob(z):\n",
    "    return -0.5 * (z.pow(2).sum(dim=1) + z.size(1) * math.log(2 * math.pi))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Enable gradient tracking for input\n",
    "        x = x.requires_grad_(True)\n",
    "        z, logdet = flow(x)\n",
    "        \n",
    "        log_pz = gaussian_log_prob(z)\n",
    "        loss = -(log_pz + logdet).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent NaN\n",
    "        torch.nn.utils.clip_grad_norm_(flow.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Sampling\n",
    "# -----------------------\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, 784).to(device)\n",
    "    # Use more iterations for better inverse approximation\n",
    "    x_gen = flow.inverse(z, n_iter=200)\n",
    "    \n",
    "    # Clamp values to valid range [0, 1]\n",
    "    x_gen = torch.clamp(x_gen, 0, 1)\n",
    "    \n",
    "    imgs = x_gen.view(-1, 1, 28, 28).cpu()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(imgs[i,0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Residual Flow Generated MNIST Samples\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/residual_flow_samples.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
